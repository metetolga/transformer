# An explanation is all we need

In the realm of natural language processing, transformer architecture from 2017 always look intimidating to me. In this repository, I've been trying to explore transformer architecture by making it from scratch and training it on a real dataset.

## Contents

* Sequential models
* Attention with Sequential models
* Transformer

I have thought seeing the sequential models would be beneficial for understanding of transformer architecture. 

## Sequential Models

## Attention with Sequential Models

## Transformer